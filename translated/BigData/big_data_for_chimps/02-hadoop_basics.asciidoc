[[hadoop_basics]]
== Hadoop 基础

=== 简介

++++
<remark>Please make sure the Chimpanzee and Elephant Start a Business big does NOT appear before this Introduction</remark>
++++

In this chapter, we will equip you with two things: the necessary mechanics of working with Hadoop, and a physical intuition for how data and computation move around the cluster during a job. 

Hadoop is a large and complex beast. It can be bewildering to even begin to use the system, and so in this chapter we're going to purposefully charge through the least you need to know to launch jobs and manage data. If you hit trouble, anything past that is well-covered in Hadoop's excellent and detailed documentation or online. But don't go looking for trouble! For every one of its many modes options and configurations that is essential, there are many more that are distracting or even dangerous. The most important optimizations you can make come from designing efficient workflows, and even more so from knowing when to spend highly valuable programmer time to reduce compute time.

The key to doing so is an intuitive, physical understanding of how data moves around a Hadoop cluster. Shipping data from one machine to another -- even from one location on disk to another -- is outrageously costly, and in the vast majority of cases dominates the cost of your job. We'll describe at a high level how Hadoop organizes data and assigns tasks across compute nodes so that as little data as possible is set in motion with both a physical analogy and by following an example job through its full lifecycle. More importantly, we'll show you how to read a job's Hadoop dashboard to understand how much it cost and why. You'll be using a docker simulation of a real Hadoop cluster on your machine. Your goal for this chapter is to take away a basic understanding of how Hadoop distributes tasks and data, and the ability to run a job and see what's going on with it. As you run more and more jobs through the remaining course of the book, it is the latter ability that will cement your intuition.

Let's kick things off by making friends with the good folks at Elephant and Chimpanzee, Inc. Their story should give you an essential physical understanding for the problems Hadoop addresses and how it solves them.

.Chimpanzee and Elephant Start a BusinessCc
******

++++
<remark>Please make sure this DOES NOT appear at the start of the chapter, before the preceding introduction.</remark>
++++

A few years back, two friends -- JT, a gruff chimpanzee, and Nanette, a meticulous matriarch elephant -- decided to start a business. As you know, Chimpanzees love nothing more than sitting at keyboards processing and generating text. Elephants have a prodigious ability to store and recall information, and will carry huge amounts of cargo with great determination. This combination of skills impressed a local publishing company enough to earn their first contract, so Chimpanzee and Elephant, Incorporated (C&E for short) was born.

The publishing firm’s project was to translate the works of Shakespeare into every language known to man, so JT and Nanette devised the following scheme. Their crew set up a large number of cubicles, each with one elephant-sized desk and several chimp-sized desks, and a command center where JT and Nanette can coordinate the action.

As with any high-scale system, each member of the team has a single responsibility to perform. The task of ǒǒeach chimpanzee is simply to read a set of passages and type out the corresponding text in a new language. JT, their foreman, efficiently assigns passages to chimpanzees, deals with absentee workers and sick days, and reports progress back to the customer. The task of each librarian elephant is to maintain a neat set of scrolls, holding either a passage to translate or some passage's translated result. Nanette serves as chief librarian. She keeps a card catalog listing, for every book, the location and essential characteristics of the various scrolls that maintain its contents. 

When workers clock in for the day, they check with JT, who hands off the day's translation manual and the name of a passage to translate. Throughout the day the chimps radio progress reports in to JT; if their assigned passage is complete, JT will specify the next passage to translate.

If you were to walk by a cubicle mid-workday, you would see a highly-efficient interplay between chimpanzee and elephant, ensuring the expert translators rarely had a wasted moment. As soon as JT radios back what passage to translate next, the elephant hands it across. The chimpanzee types up the translation on a new scroll, hands it back to its librarian partner and radios for the next passage. The librarian runs the scroll through a fax machine to send it to two of its counterparts at other cubicles, producing the redundant, triplicate copies Nanette's scheme requires. 

The librarians in turn notify Nanette which copies of which translations they hold, which helps Nanette maintain her card catalog. Whenever a customer comes calling for a translated passage, Nanette fetches all three copies and ensures they are consistent. This way, the work of each monkey can be compared to ensure its integrity, and documents can still be retrieved even if a cubicle radio fails.

The fact that each chimpanzee's work is independent of any other's -- no interoffice memos, no meetings, no requests for documents from other departments -- made this the perfect first contract for the C&E crew. JT and Nanette, however, were cooking up a new way to put their million-chimp army to work, one that could radically streamline the processes of any modern paperful office footnote:[Some chimpanzee philosophers have put forth the fanciful conceit of a "paper-less" office, requiring impossibilities like a sea of electrons that do the work of a chimpanzee, and disks of magnetized iron that would serve as scrolls. These ideas are, of course, pure lunacy!]. JT and Nanette would soon have the chance of a lifetime to try it out for a customer in the far north with a big, big problem.
******

=== Map-only 作业：单独的处理记录 ===

正如你所猜测，Chimpanzee 和 Elephant组织文件和工作流程的方式与Hadoop处理数据和后台计算正好对应。接下来我们通过一个详细的例子给你展示。

在目录树上的方案包代表业务关系数据库系统。 Nanette是Hadoop的一个主节点，主节点存储的目录树结构（类似卡片目录）和每个文件的数据节点引用（类似图书管理员）。JT是任务追踪的简称，它使每个独立的MapReduce任务进入一个非常连贯的系统并协同工作。让我们从一些尽可能简单的代码入手来阐明Hadoop是如何工作的。

我们也许没有像JT的会多种语言的黑猩猩一样聪明，但即使我们能将一个文本翻译成一种被称作_Igpay Atinlay_的语言。 footnote:[Sharp-eyed readers will note that this language is really called _Pig Latin._ That term has another name in the Hadoop universe, though, so we've chosen to call it Igpay Atinlay -- Pig Latin for "Pig Latin".]. 对于不熟悉Igpay Atinlay语言者，如下两点说明如何 http://en.wikipedia.org/wiki/Pig_latin#Rules[将标准英语翻译成Igpay Atinlay语言]:

* 如果一个词是以辅音字母或者辅音发音的字母开头，将它们移到词尾并加上“ay”，如：“happy”就变成了“appy-hay”，“chimp”就变成了”imp-chay”,进而”yes”就变成了“es-yay”。
* •	如果一个词是以元音开头，仅在词尾加上附加音节“way”即可，如：“another”就变成了“another-way”，“elephant”就变成了“elephant-way”。

<<pig_latin_translator>>  是我们的第一个Hadoop作业，即这个解释器只是把普通的文本文件解释成Igpay Atinlay语言。Hadoop作业只能够记录每一条你认为可以发生的记录，这就是Hadoop作业的最低限能。这使得很容易的知道它是如何发起一项作业；如何跟踪其进展，并通过运行时间、大量的数据迁移来评测Hadoop的性能情况。更重要的事实是，它在平常执行时也使它成为一个最重要的一个实例。通过适当的输入输出，不规则的Hadoop作业还能够进行熟练的超常作业，因此记住这点尤为关键。

让我们来看一个Python的例子，Python是一种通用的数据科学语言，你可以在命令行里通过运行一个文本文件来执行它，或者在超过拍字节的集群上执行它（无论什么原因产生一个拍字节的文本迫切需要翻译成“猪拉丁语”）。

[[pig_latin_translator]]
.Igpay Atinlay translator (ch_01/pig_latin.rb)
----
#!/usr/bin/bash

import sys, re

WORD_RE = re.compile(r"\b([bcdfghjklmnpqrstvwxz]*)([\w\']+)")
CAPITAL_RE = re.compile(r"[A-Z]")

def mapper(line):
  words = WORD_RE.findall(line)
  pig_latin_words = []
  for word in words:
    original_word = ''.join(word)
    head, tail = word
    head = 'w' if not head else head
    pig_latin_word = tail + head + 'ay'
    pig_latin_word = pig_latin_word.lower().capitalize() if CAPITAL_RE.match(pig_latin_word) else pig_latin_word.lower()
    pig_latin_words.append(pig_latin_word)
  return " ".join(pig_latin_words)

if __name__ == '__main__':
  for line in sys.stdin:
    print mapper(line)

----

[[pig_latin_translator]]
.Igpay Atinlay translator, pseudocode
----
for each line,
  recognize each word in the line
  and change it as follows:
    separate the head consonants (if any) from the tail of the word
    if there were no initial consonants, use 'w' as the head
    give the tail the same capitalization as the word
    thus changing the word to "tail-head-ay"
  end
  having changed all the words, emit the latinized version of the line
end
----

你最好在本地的一个数据子集上开始开发工作，那是因为他们执行起来更快并且更便宜。在本地执行Python脚本，只需在你的命令行里输入如下指令来执行：

------
cat /data/gold/text/gift_of_the_magi.txt|python examples/ch_01/pig_latin.py
------

可能的输出结果如下：
------
Theway agimay asway youway owknay ereway iseway enmay onderfullyway iseway enmay owhay oughtbray
iftsgay otay ethay Babeway inway ethay angermay Theyway inventedway ethay artway ofway ivinggay
Christmasway esentspray Beingway iseway eirthay iftsgay ereway onay oubtday iseway onesway
ossiblypay earingbay ethay ivilegepray ofway exchangeway inway asecay ofway uplicationday Andway
erehay Iway avehay amelylay elatedray otay youway ethay uneventfulway oniclechray ofway otway
oolishfay ildrenchay inway away atflay owhay ostmay unwiselyway acrificedsay orfay eachway otherway
ethay eatestgray easurestray ofway eirthay ousehay Butway inway away astlay ordway otay ethay iseway
ofway esethay aysday etlay itway ebay aidsay atthay ofway allway owhay ivegay iftsgay esethay otway ereway
ethay isestway Ofway allway owhay ivegay andway eceiveray iftsgay uchsay asway eythay areway isestway
Everywhereway eythay areway isestway Theyway areway ethay agimay

------

它在本地执行是如上结果，让我们看看在一个真正的Hadoop集群下执行时它是如何运行的。

注：在本地的一个数据子集上开展开发工作不仅仅是因为更快和更便宜，还有更多原因。更重要的是，然而提取一个有意义的表的子集同时也迫使你去了解你的数据及其数据间的关系。由于所有的数据在本地，会迫使首先找到好的做法“我将会用这些数据做什么”和进一步考虑“我该如何有效的去处理这些数据”。往往初学者会相信其对立面，但经验告诉我们，为准备一个子集的前期投资总是值得的，而不是从一开始就关注效率问题。

=== 集群上的数据

如果你之前阅读了Hadoop的官方文档, 那么你可能已经看到过这些广为使用的术语 _fully-distributed(完全分布模式),_ _pseudo-distributed(伪分布模式) 和 _local_（本地模式）。 他们描述了配置Hadoop集群的不同方式, 本章中如何运行这些实例和他们密切相关。
借助Docker，我们已经为您建立一套虚拟Hadoop环境, 你可以在你的电脑中通过虚拟环境开发和测试Hadoop作业运行情况, 就像在真实集群中操作一样. 你的作业通过使用称为HDFS(Hadoop分布式文件系统)的集群文件系统，运行于完全分布模式状态。

运行如下的Hadoop 命令，检查HDFS上有何变化:

------
hadoop fs -ls .
------

点号 `.` 表示 HDFS主目录 (如同你在Unix中使用 `~`)。 `hadoop fs` 命令需要传入2个参数：一个命令和一个路径, 如同*nix 命令。 除了`-ls`, `-cp`, `-mv`, `-rm`, `-cat`, `-head` 和`-tail` 同样如此操作。现在，让我们査看/data目录：

------
hadoop fs -ls /data/gold
------

你将会看到我们在整本书中使用的一些数据。

==== 运行作业 ====

首先, 我们使用命令行在之前使用的那个小文件上进行测试。 该命令并不处理数据而是调用hadoop去处理数据, 因此它的输出将会包含作业处理状态信息。

// Make sure to notice how much _longer_ it takes this elephant to squash a flea than it took to run without Hadoop.

------
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -file ./examples/ch_01/pig_latin.py -mapper ./examples/ch_01/pig_latin.py -input /data/gold/text/gift_of_the_magi.txt -output ./translation.out
------

你会看到类似如下的输出结果:

------
14/11/20 06:03:51 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
packageJobJar: [./examples/ch_01/pig_latin.py] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.2.0.jar] /tmp/streamjob829238017433781936.jar tmpDir=null
14/11/20 06:03:52 INFO client.RMProxy: Connecting to ResourceManager at rm/172.17.0.11:8032
14/11/20 06:03:52 INFO client.RMProxy: Connecting to ResourceManager at rm/172.17.0.11:8032
14/11/20 06:03:53 INFO mapred.FileInputFormat: Total input paths to process : 1
14/11/20 06:03:53 INFO mapreduce.JobSubmitter: number of splits:2
14/11/20 06:03:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1416458740373_0004
14/11/20 06:03:54 INFO impl.YarnClientImpl: Submitted application application_1416458740373_0004
14/11/20 06:03:54 INFO mapreduce.Job: The url to track the job: http://rm:8088/proxy/application_1416458740373_0004/
14/11/20 06:03:54 INFO mapreduce.Job: Running job: job_1416458740373_0004
14/11/20 06:04:00 INFO mapreduce.Job: Job job_1416458740373_0004 running in uber mode : false
14/11/20 06:04:00 INFO mapreduce.Job:  map 0% reduce 0%
14/11/20 06:04:05 INFO mapreduce.Job:  map 50% reduce 0%
14/11/20 06:04:05 INFO mapreduce.Job:  map 100% reduce 0%
14/11/20 06:04:10 INFO mapreduce.Job:  map 100% reduce 100%
14/11/20 06:04:10 INFO mapreduce.Job: Job job_1416458740373_0004 completed successfully
14/11/20 06:04:10 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=16495
		FILE: Number of bytes written=349741
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=14008
		HDFS: Number of bytes written=16039
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=2
		Launched reduce tasks=1
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=6827
		Total time spent by all reduces in occupied slots (ms)=3068
		Total time spent by all map tasks (ms)=6827
		Total time spent by all reduce tasks (ms)=3068
		Total vcore-seconds taken by all map tasks=6827
		Total vcore-seconds taken by all reduce tasks=3068
		Total megabyte-seconds taken by all map tasks=6990848
		Total megabyte-seconds taken by all reduce tasks=3141632
	Map-Reduce Framework
		Map input records=225
		Map output records=225
		Map output bytes=16039
		Map output materialized bytes=16501
		Input split bytes=204
		Combine input records=0
		Combine output records=0
		Reduce input groups=180
		Reduce shuffle bytes=16501
		Reduce input records=225
		Reduce output records=225
		Spilled Records=450
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=112
		CPU time spent (ms)=1970
		Physical memory (bytes) snapshot=685285376
		Virtual memory (bytes) snapshot=2261647360
		Total committed heap usage (bytes)=496500736
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=13804
	File Output Format Counters 
		Bytes Written=16039
14/11/20 06:04:10 INFO streaming.StreamJob: Output directory: ./translation.out
------

.作业浏览窗口
********
当你的屏幕上开始输出一系列运行状态时, 在你的浏览器窗口中打开链接 http://$CLUSTER_IP:9001/jobbrowser/ 。 你会被要求使用chimpy/chimpy 进行登录。 几秒钟后你将会看到作业运行信息。

作业浏览窗口提供了一个自带的操作台用于监视和诊断这些作业。 他是Hue的一部分 - Hadoop 图形用户界面。

image:images/01_job_browser_1.png[Hue Job Browser Interface]

你将会看到一个你刚刚执行过的作业列表。  map 和 reduce 列分别表示 mappers 和 reducers 的完成百分比。 点击作业的ID会跳转到该作业的汇总页面. 该页面的左边是作业的汇总信息: 用户, 状态, 指向日志的链接, map 和 reduce 的 数量以及作业执行时间。 你还可以查看作业执行日志, 这将有助于作业问题的调试。

image:images/01_job_browser_2.png[Hue Job Browser Interface - Job Page]

******

通过运行如下命令，你可以与之前本地运行输出结果进行对比:

------
hadoop fs -cat ./translation.out/*
------

该命令, 功能同 Unix 中的‘cat’ 命令相同, 标准输出文件的内容, 因此你可以在任何命令行实用工具中实用它。 其功能是输出整个文件的内容, 你可以在脚本中使用它，但是如果你的文件有几百MB, HDFS通常是把整个文件的内容输出到你的屏幕，这种做法并不可取。通常我们的替代方法是 使用类似Unix的 ‘head’ 或 'tail' 命令限制输出 (在本实例里, 输出最后10行)。

------
hadoop fs -cat ./translation.out/* | tail -n 20
------

既然你不想读取整个10GB的数据而仅仅想知道最终执行结果的数值是否正确, 可替代方案是使用 `hadoop fs -tail` 命令，它的作用是将文件的最后1KB输出到终端.

以下为你的输出结果中头，尾应该包含的内容:

image:images/01_pig_latin_output_1.png[Pig Latin 作业输出]

Hadoop 也实现了自己的 'head' 'tail' 命令:

------
hadoop fs -tail ./translation.out/*
------

=== 结尾

在下一章, 你将会学习到 map/reduce 作业 -- Hadoop 处理模型的强大功能. Let's start by joining JT and Nannette with their next client.

